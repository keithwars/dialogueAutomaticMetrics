# Dialgue Automatic Metrics

An Investigation into Automatic Metrics for A.I. Dialogue Systems. *- Master Thesis 2018/19*
This is a project being created in name of 'Integrated Project Mobile Development', a credit given at AP University College, Antwerp, Belgium.

## Synopsis

Interactions between humans and AI-powered platforms have seen a rise in the last decade. This surge has not only been noticed in task-oriented bots but also in chit- chat bots. The main problem with dialogues involving chatbots, is that they lack the ‘fluidity’ and the human-like factors that a human-human conversation has. This issue usually results in the system not being able to hold a decent conversation with humans, which leads to it losing their interest and for the conversation to end. Whenever a chatbot is developed, one of the few ways to check how good the system is performing is to test it on humans, resulting in delayed results and high costs. There are no decent proposed automatic metrics that can help in evaluating the chatbot on the go providing us with the ability to instantly see if the implementations have had any improvement or deterioration of the overall performance. In this study, we researched and proposed a component which can measure outputs from a chatbot against 4 different attributes resulting in empirical results when compared to the human evaluations.

## Requirements

List of Requirements

* **Environment:** Conda - on Linux, MacOS  
*Though it should be possible to built and run it on other platforms as well*
* **Hardware:** GPU with OpenCL 1.1 and OpenGL 3.1 capabilities  
*Tested only with NVIDIA.*

## Run and Execute

Clone this repository and run mainClass.ipynb through your Jupyter Notebook environment.

## License

* Makes use of the GNU Affero General Public License v3.0 License


